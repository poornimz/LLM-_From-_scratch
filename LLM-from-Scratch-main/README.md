<a href= "https://github.com/rasbt/LLMs-from-scratch">
<img src="https://th.bing.com/th/id/OIP.vyCveDNIrqO4PfFzRODbqwHaJR?rs=1&pid=ImgDetMain">
</a>

# LLM-from-Scratch


An educational repository inspired by Sebastian Raschka's *"LLM from Scratch"* book. This project aims to provide a step-by-step guide to understanding and building a Large Language Model (LLM) from the ground up. It is structured into modular components to enable learners to grasp the fundamental building blocks of modern LLMs.

## Repository Structure

The repository is organized into the following directories:

### **1. Text Sampling**
Understanding how raw text is processed for LLMs:

- **1_Tokenizer.ipynb**:  
  Introduces tokenization, a key step in processing raw text for machine learning.
- **2_Byte_Pair_Encoding.ipynb**:  
  Explains Byte Pair Encoding (BPE), a technique for creating subword vocabularies.
- **3_Data_Loader.ipynb**:  
  Demonstrates how to prepare and load datasets efficiently for training models.

### **2. Attention Mechanism**
Exploring the backbone of modern LLMs:

- **1_Simple_Attention_Mechanism.ipynb**:  
  Covers the fundamentals of attention mechanisms and their role in sequence modeling.
- **2_Self_Attention.ipynb**:  
  Explains self-attention, a crucial component of transformer architectures.
- **3_Casual_Attention.ipynb**:  
  Details causal attention for autoregressive tasks like text generation.
- **4_Multi_Head_Attention.ipynb**:  
  Introduces multi-head attention, enhancing the model's ability to focus on multiple aspects of input sequences.

### **3. GPT Architecture**
Delving into the structure of Generative Pre-trained Transformers:

- **GPT_Block.ipynb**:  
  Walks through building a GPT block, the core module of GPT-based architectures.

### **4. LLM Evaluation**
Measuring and understanding model performance:

- **Loss_Function.ipynb**:  
  Discusses the loss functions used in training LLMs and their impact on performance.

## Key Features

- **Educational Focus**:  
  Designed to simplify the complexities of LLMs, making it accessible for learners and educators.

- **Hands-on Notebooks**:  
  Each notebook includes code, visualizations, and explanations to aid understanding.

- **Inspired by Experts**:  
  Builds upon concepts from Sebastian Raschka's *"LLM from Scratch"*, ensuring high-quality content.


## Additional Resources

For more reference code and bonus materials, check out Sebastian Raschka's repository:
[LLM Resources and Code](https://github.com/rasbt/LLMs-from-scratch)


## Acknowledgements

This repository is inspired by the book *"LLM from Scratch"* by **Sebastian Raschka**, which provides an in-depth exploration of LLMs and their implementation.

## Contributions

Contributions are welcome! If you'd like to add new notebooks, improve existing ones, or fix any issues, feel free to submit a pull request.

